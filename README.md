# ğŸ“„ DocuRAG â€“ Local Retrieval Augmented Generation System

DocuRAG is a **local RAG (Retrieval-Augmented Generation) system** built using **Ollama, FAISS, and Python**.  
It allows users to upload PDF documents, generate embeddings, store them in a vector database, and query them using a locally running LLM.

This project focuses on **privacy-first, offline document intelligence** without relying on cloud-based LLM APIs.

---

## ğŸš€ Features

- ğŸ“š PDF document ingestion and text extraction  
- âœ‚ï¸ Chunked text processing with overlap  
- ğŸ§  Semantic embeddings using Sentence Transformers  
- âš¡ Fast similarity search with FAISS  
- ğŸ¤– Local LLM inference using Ollama  
- ğŸ› ï¸ LLM execution managed via Python subprocess  
- ğŸ“Š Performance testing for latency and accuracy  

---

## ğŸ—ï¸ Architecture Overview
PDFs â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ FAISS Vector Store
â†“
Query Embedding
â†“
Top-K Similar Chunks
â†“
Ollama Local LLM
â†“
Final Answer


---

## ğŸ§° Tech Stack

- **Python**
- **Ollama** (local LLM inference)
- **FAISS** (vector similarity search)
- **Sentence-Transformers** (`all-MiniLM-L6-v2`)
- **PyPDF**
- **Subprocess orchestration**

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ ingest.py # PDF ingestion and FAISS index creation
â”œâ”€â”€ query.py # Semantic search over vector store
â”œâ”€â”€ query_with_ai.py # RAG pipeline with Ollama LLM
â”œâ”€â”€ test_metrics.py # Latency testing for LLM inference
â”œâ”€â”€ test_ollama.py # Basic Ollama connectivity test
â”œâ”€â”€ test_ollama_query.py # Sample Ollama query test
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ vectorstore/
â”‚ â”œâ”€â”€ index.faiss
â”‚ â””â”€â”€ metadata.pkl
â””â”€â”€ data/ # PDF documents


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt


